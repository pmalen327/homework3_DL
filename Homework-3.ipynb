{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f4fd96",
   "metadata": {},
   "source": [
    "# MTH 4320 / 5320 - Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef79929",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks and PyTorch\n",
    "\n",
    "\n",
    "**Deadline**: Nov 7, submit in Canvas\n",
    "\n",
    "**Points**: 75\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Submit **one** Jupyter notebook file and (optionally) **one** PDF with your handwritten work. (Alternatively, type solution in markdown cells in the notebook.)\n",
    "\n",
    "* Your notebook file must include text explanations of your work, well-commented code, and the outputs from your code (must be shown as code output in the notebook).\n",
    "\n",
    "* All mathematical work must be shown for written/typed problems.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework you will implement, tune, and compare three CNN approaches for image classification:\n",
    "\n",
    "1. Custom CNN (from scratch) — a network you design.\n",
    "\n",
    "2. Transfer Learning CNN — fine-tune a pretrained backbone from torchvision or others.\n",
    "\n",
    "3. Ensemble — combine the Custom and Transfer models at inference time.\n",
    "\n",
    "You will report training/validation curves, final test accuracy, confusion matrices (raw + normalized), per-class accuracy, and a short written analysis comparing methods.\n",
    "\n",
    "> **Not allowed:** fully-connected networks  \n",
    "> **Required framework:** PyTorch\n",
    "> **Recommended:** GPU computing\n",
    "\n",
    "## Dataset\n",
    "\n",
    "* **Dataset:** [Intel Image Classification dataset](https://www.kaggle.com/datasets/puneet6060/intel-image-classification) — contains labeled images across multiple scene classes (e.g., `buildings`, `forest`, `street`, etc.).  \n",
    "* **Framework:** PyTorch + torchvision + matplotlib.  \n",
    "* **Data splits:** Use the provided data splits.\n",
    "\n",
    "\n",
    "## Tasks & Requirements\n",
    "\n",
    "You will complete two main problems, each requiring at least 10 independent training runs (e.g., different hyperparameters, architectures, or augmentation settings). Record all results, pick your best model, and analyze performance.\n",
    "\n",
    "Problem 3 will ask you to ensemble the models from Problems 1-2 to hopefully eke out some additional accuracy.\n",
    "\n",
    "### Problem 1 — Custom CNN [30 points]\n",
    "Train and evaluate your own CNN **from scratch** (in PyTorch) on the Intel Image Classification dataset.\n",
    "\n",
    "**Requirements:**\n",
    "- Conduct a systematic hyperparameter tuning campaign with ≥ 10 total training runs*.\n",
    "  - Each run should modify a *single well-motivated factor* (e.g., learning rate, regularization, architecture, or augmentation strategy) and include a brief rationale.\n",
    "  - Maintain a structured table summarizing all runs, backbones, hyperparameters, and accuracies.\n",
    "- Identify and report your best-performing configuration, including:\n",
    "  - training/validation accuracy and loss curves\n",
    "  - final test accuracy\n",
    "  - confusion matrix\n",
    "  - per-class accuracy summary\n",
    "- Provide a paragraph (~1/2 a page) reflecting on your tuning process: what helped, what didn’t, and how you would refine it further.\n",
    "- Save important checkpoints from all models for use in Problem 3.\n",
    "\n",
    "###  Problem 2 — Transfer Learning [30 points]\n",
    "\n",
    "Fine-tune a **pretrained CNN backbone** from `torchvision`.\n",
    "\n",
    "**Requirements:**\n",
    "- Replace the classifier head with the correct number of classes for the Intel dataset.\n",
    "- Define and justify your fine-tuning strategy (which layers frozen/unfrozen, when, and why).\n",
    "- Carry out a hyperparameter tuning campaign with ≥ 10 total runs, systematically exploring learning-rate schedules, layer-freezing policies, and/or augmentation strategies.\n",
    "  - Each experiment must have a clear, written motivation.\n",
    "  - Maintain a structured table summarizing all runs, backbones, hyperparameters, and accuracies.\n",
    "- Identify and report your best-performing configuration, including:\n",
    "  - training/validation accuracy and loss curves  \n",
    "  - final test accuracy\n",
    "  - confusion matrix\n",
    "  - per-class accuracy summary\n",
    "- Provide a paragraph (~1/2 a page) reflecting on your tuning process: what helped, what didn’t, and how you would refine it further.\n",
    "- Save important checkpoints from all models for use in Problem 3.\n",
    "\n",
    "### Problem 3 — Model Ensembling and Performance Integration [15 points]\n",
    "\n",
    "In this final problem, you will combine and evaluate ensembles built from models trained during your hyperparameter tuning campaigns in Problems 1 and 2. Rather than training new networks, you will use your saved checkpoints (best runs or diverse configurations) to study how ensembling affects performance, generalization, and class-wise stability.\n",
    "\n",
    "The goal is to understand how architectural diversity and independent tuning decisions interact when multiple models vote or average predictions.  \n",
    "\n",
    "You will test several ensemble strategies, compare them to your best single models, and analyze which factors make ensembles most effective.\n",
    "\n",
    "**Requirements:**\n",
    "- Build ensembles using models saved from your tuning campaigns in Problems 1 and 2 (do not retrain).  \n",
    "  - Use at least 5 high-performing checkpoints, mixing both Custom CNN and Transfer Learning models.\n",
    "- Explore at least two ensemble strategies (such as hard vs. soft voting; uniform vs. weighted averaging)\n",
    "- Evaluate each ensemble on the same held-out test set and report:\n",
    "  - training/validation accuracy and loss curves\n",
    "  - final test accuracy\n",
    "  - confusion matrix\n",
    "  - per-class accuracy summary\n",
    "- Present a summary table listing ensemble composition, weighting scheme, and performance metrics for all ensemble variants tested.\n",
    "- Write a discussion (~1/2 page) of describing how ensembling affected accuracy and stability across classes, whether model diversity (architecture or hyperparameters) improved results, and which ensemble approach was most effective and why\n",
    "- Conclude with a brief reflection (3–5 sentences) on what your findings reveal about model diversity, robustness, and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096310fa",
   "metadata": {},
   "source": [
    "## Checklist\n",
    "\n",
    "- [ ] Custom CNN, Transfer model, and Ensemble implemented  \n",
    "- [ ] Curves (acc/loss), confusion matrices (raw + normalized), per‑class accuracy  \n",
    "- [ ] Final test accuracies reported for all three models  \n",
    "- [ ] Summary & Reflection (≤ 300 words) written  \n",
    "- [ ] Seed, versions, and hardware documented  \n",
    "- [ ] Notebook runs cleanly end‑to‑end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
